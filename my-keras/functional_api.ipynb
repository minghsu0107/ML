{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "functional_api.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "hdx1ApYXdfso",
        "eQcK3prjfbqz",
        "GcV-p_nQltpq",
        "GCjyS9QOq5PZ",
        "Ih0btulGxoDZ",
        "RVJP0s54fMN8",
        "Kf3xj55hLb7N",
        "ZlOvNq0MQj1G"
      ],
      "toc_visible": true,
      "mount_file_id": "1iAR3B1AJgvNnj0JNyMipWr63wYKzhMkO",
      "authorship_tag": "ABX9TyOVZ/uKFWYtxqBbt3rEhHor",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/minghsu0107/ML/blob/master/my-keras/functional_api.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdx1ApYXdfso",
        "colab_type": "text"
      },
      "source": [
        "# Intro"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zh5KIy5scYva",
        "colab_type": "text"
      },
      "source": [
        "To handle tasks of multiple inputs or outputs, we need to build models that are not sequentially stacked."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "um0TA70ldbGD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "outputId": "362b786e-9130-4013-daad-645dab25f5a2"
      },
      "source": [
        "from keras.models import Sequential, Model\n",
        "from keras import layers, Input\n",
        "\n",
        "seq_model = Sequential()\n",
        "seq_model.add(layers.Dense(32, activation='relu', input_shape=(64,)))\n",
        "seq_model.add(layers.Dense(32, activation='relu'))\t\t\t\t\t   #1...\n",
        "seq_model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "input_tensor = Input(shape=(64,))   #← 建立一個初始張量\n",
        "\n",
        "# 將初始張量傳入 Dense 層得到輸出張量 x\n",
        "x = layers.Dense(32, activation='relu')(input_tensor)\n",
        " \n",
        "# 再將第一層的結果 x 傳入第 2 個 Dense 層得到輸出張量 y                2...\n",
        "y = layers.Dense(32, activation='relu')(x) \n",
        "\n",
        "# 再將第二層的結果 y 傳入最後一個 Dense 層得到最後的輸出張量 output_tensor\n",
        "output_tensor = layers.Dense(10, activation='softmax')(y) \n",
        "\n",
        "# Model 類別 \"用\" 初始的輸入張量和最後的輸出張量來得到模型物件\n",
        "model = Model(input_tensor, output_tensor)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 64)]              0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 32)                2080      \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 10)                330       \n",
            "=================================================================\n",
            "Total params: 3,466\n",
            "Trainable params: 3,466\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQcK3prjfbqz",
        "colab_type": "text"
      },
      "source": [
        "# 以函數式 API 實作雙輸入問答模型"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DrnVklefcUG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "2687cd19-8057-40d3-ce9d-94c8b05a0508"
      },
      "source": [
        "from keras import Model\n",
        "from keras import layers\n",
        "from keras import Input\n",
        "\n",
        "text_vocabulary_size = 10000\n",
        "question_vocabulary_size = 10000\n",
        "answer_vocabulary_size = 500 # 對於某些事件先定義好詞彙\n",
        "\t\t\t\t\t\t #↓1...                   #↓2...\n",
        "text_input = Input(shape=(None, ), dtype='int32', name='text') \n",
        "embedded_text = layers.Embedding(text_vocabulary_size, 64)(text_input) #← 3...\n",
        "print(embedded_text.shape)  \t#→ (?, ?, 64)\n",
        "encoded_text = layers.LSTM(32)(embedded_text) #← 4...\n",
        "print(encoded_text.shape)  #\t→ (?, 32)\n",
        "\n",
        "question_input = Input(shape=(None, ), dtype='int32', name='question')\n",
        "embedded_question = layers.Embedding(question_vocabulary_size, 32)(question_input) #5..\n",
        "print(embedded_question.shape)  \t#→ (?, ?, 32)\n",
        "encoded_question = layers.LSTM(16)(embedded_question)\n",
        "print(encoded_question.shape)  \t#→ (?, 16)\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t#↓6...\n",
        "concatenated = layers.concatenate([encoded_question, encoded_text], axis=-1) \n",
        "print(concatenated.shape)  #→ (?, 48)\n",
        "\n",
        "answer = layers.Dense(answer_vocabulary_size, activation='softmax')(concatenated) #← 7...\n",
        "print(answer.shape)  #→ (?, 500) \n",
        "\n",
        "model = Model([text_input, question_input], answer) #← 8...\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n",
        "model.summary()\n",
        "\n",
        "#1. shape = (None, ) 代表不限定張量的 shape 大小, 所以文字輸入可以是可變長度的整數序列。\n",
        "#2. 請注意, 可以選擇是否為輸入命名\n",
        "#3. 將輸入送進嵌入層, 編碼成大小 64 的文字嵌入向量 (處理 「參考文字」輸入)。\n",
        "#4. 再透過 LSTM 層將向量序列編碼成單一個向量\n",
        "#5. 處理「問題」輸入的流程 (與處理「參考文字」輸入的流程相同)\n",
        "#6. 串接編碼後的「問題」和「參考文字」資料 (向量), 將兩份資料合而為一。axis 參數為 -1 代表以輸入的最後一個軸進行串接。\n",
        "#7. 最後增加一個 Dense層 (softmax分類器), 將串接向量送入, 輸出模型的結果張量 answer。\n",
        "#8. 在模型實例化時, 因為有兩個輸入, 所以將它們組成一個 list 一起做為輸入, 而輸出為 answer。"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(None, None, 64)\n",
            "(None, 32)\n",
            "(None, None, 32)\n",
            "(None, 16)\n",
            "(None, 48)\n",
            "(None, 500)\n",
            "Model: \"functional_3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "question (InputLayer)           [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "text (InputLayer)               [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, None, 32)     320000      question[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, None, 64)     640000      text[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 16)           3136        embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     (None, 32)           12416       embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 48)           0           lstm_1[0][0]                     \n",
            "                                                                 lstm[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 500)          24500       concatenate[0][0]                \n",
            "==================================================================================================\n",
            "Total params: 1,000,052\n",
            "Trainable params: 1,000,052\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbn603gxff2I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "599b4991-82c4-4ff9-b697-f36347a882da"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "num_samples = 1000\n",
        "max_length = 100\n",
        "\n",
        "# 產生 text 資料：1000 筆, 每筆 100 個字 (數字)\n",
        "text = np.random.randint(1, text_vocabulary_size, \n",
        "                         size=(num_samples, max_length))\n",
        "#  [  [2, 15, 8000,..... 共 100 個], [],....共 1000 筆  ]  \n",
        "#      ↑   ↑    ↑         \n",
        "#     產生 1 ~ 10000 (text_vocabulary_size) 區間的數字 \n",
        "print(text.shape)       # (1000, 100)\n",
        "\n",
        "# 產生 question 資料, 與上面 text 產生方式相同\n",
        "question = np.random.randint(1, question_vocabulary_size, \n",
        "                             size=(num_samples, max_length))\n",
        "print(question.shape)   # (1000, 100)\n",
        "\n",
        "# 產生 answers 資料, 需為 One-hot 編碼, 共 1000 個正確答案\n",
        "answers = np.random.randint(0, 1, size=(num_samples, \n",
        "                                        answer_vocabulary_size))\n",
        "#  [  [0, 1, 1,..... 共 100 個], [],.... 共 1000 筆  ]\n",
        "#      ↑  ↑  ↑         \n",
        "#     產生 0 ~ 1 的數字 \n",
        "# 此為分類器要用的 One-encoding 編碼答案    \n",
        "print(answers.shape)    # (1000, 500)\n",
        "\n",
        "# 訓練方法 1：使用 list 方式送入資料進行擬合 \n",
        "#model.fit([text, question], answers, epochs=10, batch_size=128)\n",
        "# 訓練方法 2：使用 dict 方式送入資料進行擬合, 鍵為 Input 層的名稱, 值為 Numpy 資料\n",
        "model.fit({'text': text, 'question': question}, answers, epochs=10,  batch_size=128)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1000, 100)\n",
            "(1000, 100)\n",
            "(1000, 500)\n",
            "Epoch 1/10\n",
            "8/8 [==============================] - 0s 25ms/step - loss: 0.0000e+00 - acc: 0.0050\n",
            "Epoch 2/10\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.0000e+00 - acc: 0.0000e+00\n",
            "Epoch 3/10\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.0000e+00 - acc: 0.0000e+00\n",
            "Epoch 4/10\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.0000e+00 - acc: 0.0000e+00\n",
            "Epoch 5/10\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.0000e+00 - acc: 0.0000e+00\n",
            "Epoch 6/10\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.0000e+00 - acc: 0.0000e+00\n",
            "Epoch 7/10\n",
            "8/8 [==============================] - 0s 20ms/step - loss: 0.0000e+00 - acc: 0.0000e+00\n",
            "Epoch 8/10\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.0000e+00 - acc: 0.0000e+00\n",
            "Epoch 9/10\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.0000e+00 - acc: 0.0000e+00\n",
            "Epoch 10/10\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.0000e+00 - acc: 0.0000e+00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f34d334fc88>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcV-p_nQltpq",
        "colab_type": "text"
      },
      "source": [
        "# 多輸出模型"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7ZiYZyzlwvL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 642
        },
        "outputId": "cbac7271-a3b0-42f1-b7a1-33e78dbd6b03"
      },
      "source": [
        "from keras import layers, Input\n",
        "from keras.models import Model\n",
        "\n",
        "vocabulary_size = 50000 \t#← 文章大小\n",
        "num_income_groups = 10 \t#← 將收入分成 10 群\n",
        "                            \n",
        "                          # ↓不限定輸入向量的 shape 大小\n",
        "posts_input = Input(shape=(None,), dtype='int32', name='posts') \n",
        "\n",
        "# 用函數式 API 將輸入向量傳入 Embedding 層, 得到維度 256 的嵌入向量\n",
        "embedding_posts = layers.Embedding(vocabulary_size, 256)(posts_input)\n",
        "print(embedding_posts.shape)   # ← (?, ?, 256)\n",
        "\n",
        "# 以下以函數式 API 將嵌入向量傳入一層層之中進行處理\n",
        "x = layers.Conv1D(128, 5, activation='relu')(embedding_posts)\n",
        "x = layers.MaxPooling1D(5)(x)\n",
        "x = layers.Conv1D(256, 5, activation='relu')(x)\n",
        "x = layers.Conv1D(256, 5, activation='relu')(x)\n",
        "x = layers.MaxPooling1D(5)(x)\n",
        "x = layers.Conv1D(256, 5, activation='relu')(x)\n",
        "x = layers.Conv1D(256, 5, activation='relu')(x)\n",
        "x = layers.GlobalMaxPooling1D()(x)  \n",
        "x = layers.Dense(128, activation='relu')(x)\n",
        "print(x.shape)  #← 走過一連串層之後, x.shape 為 (?, 128)\n",
        "\n",
        "# 接下來將 x 向量分別送入 3 個輸出層。請注意, \n",
        "# 需為輸出層指定名稱(原因請見程式 7.5 中的編譯方法 2)\n",
        "\n",
        "# 預測年紀的輸出層：純量迴歸任務\n",
        "age_prediction = layers.Dense(1, name='age')(x)\n",
        "\n",
        "# 預測收入族群的輸出層多分類任務 (10 類)\n",
        "income_prediction = layers.Dense(num_income_groups, \n",
        "                                 activation='softmax', \n",
        "                                 name='income')(x)\n",
        "# 預測性別的輸出層：二元分類任務\n",
        "gender_prediction = layers.Dense(1, \n",
        "                                 activation='sigmoid', \n",
        "                                 name='gender')(x)\n",
        "\n",
        "# 用輸入向量與輸出向量實例化 Model 物件\n",
        "model = Model(posts_input, \n",
        "              [age_prediction, income_prediction, gender_prediction])\n",
        "                 #↑ 因為輸出向量有 3 個, 所以用 list 來組成\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(None, None, 256)\n",
            "(None, 128)\n",
            "Model: \"functional_5\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "posts (InputLayer)              [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_2 (Embedding)         (None, None, 256)    12800000    posts[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d (Conv1D)                 (None, None, 128)    163968      embedding_2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d (MaxPooling1D)    (None, None, 128)    0           conv1d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, None, 256)    164096      max_pooling1d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, None, 256)    327936      conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1D)  (None, None, 256)    0           conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, None, 256)    327936      max_pooling1d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_4 (Conv1D)               (None, None, 256)    327936      conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d (GlobalMax (None, 256)          0           conv1d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_7 (Dense)                 (None, 128)          32896       global_max_pooling1d[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "age (Dense)                     (None, 1)            129         dense_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "income (Dense)                  (None, 10)           1290        dense_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "gender (Dense)                  (None, 1)            129         dense_7[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 14,146,316\n",
            "Trainable params: 14,146,316\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0hXrm_9pL0G",
        "colab_type": "text"
      },
      "source": [
        "若有不平衡的損失會導致模型優先針對最大損失的任務優化，而犧牲其他任務。此時可以使用 loss_weights 為損失值分配不同程度的重要性。尤其當損失值使用不同單位時特別有用。比如 MSE 通常取值在 3 ~ 5，而 cross-entropy 則可低至 0.1，所以我們可以為 cross-entropy 配置 10 的權重，並為 MSE 配置 0.25 權重。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKGfEsP_opci",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 編譯方式 1 \n",
        "model.compile(optimizer='rmsprop', \n",
        "              loss=['mse',\t\t#← (需照建立層的順序)\n",
        "                    'categorical_crossentropy', \n",
        "                    'binary_crossentropy'],\n",
        "              loss_weights=[0.25, 1., 10.])\n",
        "# 編譯方式 2 \n",
        "model.compile(optimizer='rmsprop', \n",
        "              loss={'age': 'mse',\t#← (需為輸出層指定名稱)\n",
        "                    'income': 'categorical_crossentropy', \n",
        "                    'gender': 'binary_crossentropy'},\n",
        "              loss_weights={'age': 0.25,\n",
        "                            'income': 1.,\n",
        "                            'gender': 10.})\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5EoG4yGwpKo6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3398ac0-6b4b-4595-99b9-33a722283bf1"
      },
      "source": [
        "# first way\n",
        "model.fit(posts,\n",
        "          [age_targets, income_targets, gender_targets],\n",
        "          epochs=10,\n",
        "          batch_size=64)\n",
        "# second way\n",
        "model.fit(posts,\n",
        "          {'age', age_targets, \n",
        "           'income', income_targets, \n",
        "           'gender', gender_targets},\n",
        "          epochs=10,\n",
        "          batch_size=64)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-b299d04224e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model.fit(posts,\n\u001b[0m\u001b[1;32m      2\u001b[0m           \u001b[0;34m[\u001b[0m\u001b[0mage_targets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincome_targets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgender_targets\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m           batch_size=64)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'posts' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCjyS9QOq5PZ",
        "colab_type": "text"
      },
      "source": [
        "# Inception"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLKUYOMFn5SY",
        "colab_type": "text"
      },
      "source": [
        "The extracted feature channels do not contain space information if we use 1 x 1 Conv layer in the beginning. In Inception, this helps separate channel features from space features (assuming that every channel is highly related to the space information and channels are not mutually related).\n",
        "\n",
        "On the other hand, 1 x 1 Conv layer also plays a role in performing dimension reduction. For example, a (28, 28, 192) tensor processed by 5 x 5 conv kernel with a (28, 28, 32) output takes (28 x 28 x 192) x (5 x 5 x 32) = 120422400 operations. However, if we reduce the dimension to (28, 28, 16) by 1 x 1 conv layer first, it only takes (28 x 28 x 192) x (1 x 1 x 16) + (28 x 28 x 16) x (5 x 5 x 32) = 12443648 operations. Now we have reduced the computation time down to 1/10. Therefore, 1 x 1 conv layer is also called the bottleneck layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fiSoUnO9jLsV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "outputId": "e63c7370-9c55-4f04-d30a-fa7c69f92612"
      },
      "source": [
        "from keras import layers, Input\n",
        "\n",
        "x = Input(batch_shape=(1000, 28, 28, 256))\n",
        "\n",
        "branch_a = layers.Conv2D(64, 1, activation='relu', strides=2)(x) # stides=2: height and width become half\n",
        "print(branch_a.shape) # (1000, 14, 14, 64)\n",
        "\n",
        "branch_b = layers.Conv2D(128, 1, activation='relu')(x) # by default, strides=1 and no padding\n",
        "# padding='same': 用zero-padding的手法，讓輸入的圖不會受到kernel map的大小影響\n",
        "branch_b = layers.Conv2D(128, 3, activation='relu', strides=2, padding='same')(branch_b)\n",
        "print(branch_b.shape) # (1000, 14, 14, 128)\n",
        "\n",
        "branch_c = layers.AveragePooling2D(3, strides=2, padding='same')(x)\n",
        "print(branch_c.shape) # (1000, 14, 14, 256)\n",
        "branch_c = layers.Conv2D(128, 3, activation='relu', padding='same')(branch_c)\n",
        "print(branch_c.shape) # (1000, 14, 14, 128)\n",
        "\n",
        "branch_d = layers.Conv2D(128, 1, activation='relu')(x)\n",
        "branch_d = layers.Conv2D(128, 3, activation='relu', padding='same')(branch_d)\n",
        "print(branch_d.shape) # (1000, 28, 28, 128)\n",
        "branch_d = layers.Conv2D(128, 3, activation='relu', strides=2, padding='same')(branch_d)\n",
        "print(branch_d.shape) # (1000, 14, 14, 128)\n",
        "\n",
        "output = layers.concatenate([branch_a, branch_b, branch_c, branch_d], axis=-1) # concatenate by the last dimension\n",
        "print(output.shape) # (1000, 14, 14, 448)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1000, 14, 14, 64)\n",
            "(1000, 14, 14, 128)\n",
            "(1000, 14, 14, 256)\n",
            "(1000, 14, 14, 128)\n",
            "(1000, 28, 28, 128)\n",
            "(1000, 14, 14, 128)\n",
            "(1000, 14, 14, 448)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-Zv1u35uzkp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3b05ff83-de99-447e-d37f-deeaabe8cf0c"
      },
      "source": [
        "from keras.applications.inception_v3 import InceptionV3\n",
        "\n",
        "conv_base = InceptionV3(weights='imagenet',\n",
        "                        include_top=False, # do not include the top dense layer (1000 categories)\n",
        "                        input_shape=(224, 224, 3))\n",
        "\n",
        "conv_base.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"inception_v3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_8 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_484 (Conv2D)             (None, 111, 111, 32) 864         input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_470 (BatchN (None, 111, 111, 32) 96          conv2d_484[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_470 (Activation)     (None, 111, 111, 32) 0           batch_normalization_470[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_485 (Conv2D)             (None, 109, 109, 32) 9216        activation_470[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_471 (BatchN (None, 109, 109, 32) 96          conv2d_485[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_471 (Activation)     (None, 109, 109, 32) 0           batch_normalization_471[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_486 (Conv2D)             (None, 109, 109, 64) 18432       activation_471[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_472 (BatchN (None, 109, 109, 64) 192         conv2d_486[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_472 (Activation)     (None, 109, 109, 64) 0           batch_normalization_472[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_20 (MaxPooling2D) (None, 54, 54, 64)   0           activation_472[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_487 (Conv2D)             (None, 54, 54, 80)   5120        max_pooling2d_20[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_473 (BatchN (None, 54, 54, 80)   240         conv2d_487[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_473 (Activation)     (None, 54, 54, 80)   0           batch_normalization_473[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_488 (Conv2D)             (None, 52, 52, 192)  138240      activation_473[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_474 (BatchN (None, 52, 52, 192)  576         conv2d_488[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_474 (Activation)     (None, 52, 52, 192)  0           batch_normalization_474[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_21 (MaxPooling2D) (None, 25, 25, 192)  0           activation_474[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_492 (Conv2D)             (None, 25, 25, 64)   12288       max_pooling2d_21[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_478 (BatchN (None, 25, 25, 64)   192         conv2d_492[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_478 (Activation)     (None, 25, 25, 64)   0           batch_normalization_478[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_490 (Conv2D)             (None, 25, 25, 48)   9216        max_pooling2d_21[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_493 (Conv2D)             (None, 25, 25, 96)   55296       activation_478[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_476 (BatchN (None, 25, 25, 48)   144         conv2d_490[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_479 (BatchN (None, 25, 25, 96)   288         conv2d_493[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_476 (Activation)     (None, 25, 25, 48)   0           batch_normalization_476[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_479 (Activation)     (None, 25, 25, 96)   0           batch_normalization_479[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_47 (AveragePo (None, 25, 25, 192)  0           max_pooling2d_21[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_489 (Conv2D)             (None, 25, 25, 64)   12288       max_pooling2d_21[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_491 (Conv2D)             (None, 25, 25, 64)   76800       activation_476[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_494 (Conv2D)             (None, 25, 25, 96)   82944       activation_479[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_495 (Conv2D)             (None, 25, 25, 32)   6144        average_pooling2d_47[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_475 (BatchN (None, 25, 25, 64)   192         conv2d_489[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_477 (BatchN (None, 25, 25, 64)   192         conv2d_491[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_480 (BatchN (None, 25, 25, 96)   288         conv2d_494[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_481 (BatchN (None, 25, 25, 32)   96          conv2d_495[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_475 (Activation)     (None, 25, 25, 64)   0           batch_normalization_475[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_477 (Activation)     (None, 25, 25, 64)   0           batch_normalization_477[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_480 (Activation)     (None, 25, 25, 96)   0           batch_normalization_480[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_481 (Activation)     (None, 25, 25, 32)   0           batch_normalization_481[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed0 (Concatenate)            (None, 25, 25, 256)  0           activation_475[0][0]             \n",
            "                                                                 activation_477[0][0]             \n",
            "                                                                 activation_480[0][0]             \n",
            "                                                                 activation_481[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_499 (Conv2D)             (None, 25, 25, 64)   16384       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_485 (BatchN (None, 25, 25, 64)   192         conv2d_499[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_485 (Activation)     (None, 25, 25, 64)   0           batch_normalization_485[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_497 (Conv2D)             (None, 25, 25, 48)   12288       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_500 (Conv2D)             (None, 25, 25, 96)   55296       activation_485[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_483 (BatchN (None, 25, 25, 48)   144         conv2d_497[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_486 (BatchN (None, 25, 25, 96)   288         conv2d_500[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_483 (Activation)     (None, 25, 25, 48)   0           batch_normalization_483[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_486 (Activation)     (None, 25, 25, 96)   0           batch_normalization_486[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_48 (AveragePo (None, 25, 25, 256)  0           mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_496 (Conv2D)             (None, 25, 25, 64)   16384       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_498 (Conv2D)             (None, 25, 25, 64)   76800       activation_483[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_501 (Conv2D)             (None, 25, 25, 96)   82944       activation_486[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_502 (Conv2D)             (None, 25, 25, 64)   16384       average_pooling2d_48[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_482 (BatchN (None, 25, 25, 64)   192         conv2d_496[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_484 (BatchN (None, 25, 25, 64)   192         conv2d_498[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_487 (BatchN (None, 25, 25, 96)   288         conv2d_501[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_488 (BatchN (None, 25, 25, 64)   192         conv2d_502[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_482 (Activation)     (None, 25, 25, 64)   0           batch_normalization_482[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_484 (Activation)     (None, 25, 25, 64)   0           batch_normalization_484[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_487 (Activation)     (None, 25, 25, 96)   0           batch_normalization_487[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_488 (Activation)     (None, 25, 25, 64)   0           batch_normalization_488[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed1 (Concatenate)            (None, 25, 25, 288)  0           activation_482[0][0]             \n",
            "                                                                 activation_484[0][0]             \n",
            "                                                                 activation_487[0][0]             \n",
            "                                                                 activation_488[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_506 (Conv2D)             (None, 25, 25, 64)   18432       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_492 (BatchN (None, 25, 25, 64)   192         conv2d_506[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_492 (Activation)     (None, 25, 25, 64)   0           batch_normalization_492[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_504 (Conv2D)             (None, 25, 25, 48)   13824       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_507 (Conv2D)             (None, 25, 25, 96)   55296       activation_492[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_490 (BatchN (None, 25, 25, 48)   144         conv2d_504[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_493 (BatchN (None, 25, 25, 96)   288         conv2d_507[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_490 (Activation)     (None, 25, 25, 48)   0           batch_normalization_490[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_493 (Activation)     (None, 25, 25, 96)   0           batch_normalization_493[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_49 (AveragePo (None, 25, 25, 288)  0           mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_503 (Conv2D)             (None, 25, 25, 64)   18432       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_505 (Conv2D)             (None, 25, 25, 64)   76800       activation_490[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_508 (Conv2D)             (None, 25, 25, 96)   82944       activation_493[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_509 (Conv2D)             (None, 25, 25, 64)   18432       average_pooling2d_49[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_489 (BatchN (None, 25, 25, 64)   192         conv2d_503[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_491 (BatchN (None, 25, 25, 64)   192         conv2d_505[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_494 (BatchN (None, 25, 25, 96)   288         conv2d_508[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_495 (BatchN (None, 25, 25, 64)   192         conv2d_509[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_489 (Activation)     (None, 25, 25, 64)   0           batch_normalization_489[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_491 (Activation)     (None, 25, 25, 64)   0           batch_normalization_491[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_494 (Activation)     (None, 25, 25, 96)   0           batch_normalization_494[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_495 (Activation)     (None, 25, 25, 64)   0           batch_normalization_495[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed2 (Concatenate)            (None, 25, 25, 288)  0           activation_489[0][0]             \n",
            "                                                                 activation_491[0][0]             \n",
            "                                                                 activation_494[0][0]             \n",
            "                                                                 activation_495[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_511 (Conv2D)             (None, 25, 25, 64)   18432       mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_497 (BatchN (None, 25, 25, 64)   192         conv2d_511[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_497 (Activation)     (None, 25, 25, 64)   0           batch_normalization_497[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_512 (Conv2D)             (None, 25, 25, 96)   55296       activation_497[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_498 (BatchN (None, 25, 25, 96)   288         conv2d_512[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_498 (Activation)     (None, 25, 25, 96)   0           batch_normalization_498[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_510 (Conv2D)             (None, 12, 12, 384)  995328      mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_513 (Conv2D)             (None, 12, 12, 96)   82944       activation_498[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_496 (BatchN (None, 12, 12, 384)  1152        conv2d_510[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_499 (BatchN (None, 12, 12, 96)   288         conv2d_513[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_496 (Activation)     (None, 12, 12, 384)  0           batch_normalization_496[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_499 (Activation)     (None, 12, 12, 96)   0           batch_normalization_499[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_22 (MaxPooling2D) (None, 12, 12, 288)  0           mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "mixed3 (Concatenate)            (None, 12, 12, 768)  0           activation_496[0][0]             \n",
            "                                                                 activation_499[0][0]             \n",
            "                                                                 max_pooling2d_22[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_518 (Conv2D)             (None, 12, 12, 128)  98304       mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_504 (BatchN (None, 12, 12, 128)  384         conv2d_518[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_504 (Activation)     (None, 12, 12, 128)  0           batch_normalization_504[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_519 (Conv2D)             (None, 12, 12, 128)  114688      activation_504[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_505 (BatchN (None, 12, 12, 128)  384         conv2d_519[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_505 (Activation)     (None, 12, 12, 128)  0           batch_normalization_505[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_515 (Conv2D)             (None, 12, 12, 128)  98304       mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_520 (Conv2D)             (None, 12, 12, 128)  114688      activation_505[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_501 (BatchN (None, 12, 12, 128)  384         conv2d_515[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_506 (BatchN (None, 12, 12, 128)  384         conv2d_520[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_501 (Activation)     (None, 12, 12, 128)  0           batch_normalization_501[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_506 (Activation)     (None, 12, 12, 128)  0           batch_normalization_506[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_516 (Conv2D)             (None, 12, 12, 128)  114688      activation_501[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_521 (Conv2D)             (None, 12, 12, 128)  114688      activation_506[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_502 (BatchN (None, 12, 12, 128)  384         conv2d_516[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_507 (BatchN (None, 12, 12, 128)  384         conv2d_521[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_502 (Activation)     (None, 12, 12, 128)  0           batch_normalization_502[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_507 (Activation)     (None, 12, 12, 128)  0           batch_normalization_507[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_50 (AveragePo (None, 12, 12, 768)  0           mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_514 (Conv2D)             (None, 12, 12, 192)  147456      mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_517 (Conv2D)             (None, 12, 12, 192)  172032      activation_502[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_522 (Conv2D)             (None, 12, 12, 192)  172032      activation_507[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_523 (Conv2D)             (None, 12, 12, 192)  147456      average_pooling2d_50[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_500 (BatchN (None, 12, 12, 192)  576         conv2d_514[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_503 (BatchN (None, 12, 12, 192)  576         conv2d_517[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_508 (BatchN (None, 12, 12, 192)  576         conv2d_522[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_509 (BatchN (None, 12, 12, 192)  576         conv2d_523[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_500 (Activation)     (None, 12, 12, 192)  0           batch_normalization_500[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_503 (Activation)     (None, 12, 12, 192)  0           batch_normalization_503[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_508 (Activation)     (None, 12, 12, 192)  0           batch_normalization_508[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_509 (Activation)     (None, 12, 12, 192)  0           batch_normalization_509[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed4 (Concatenate)            (None, 12, 12, 768)  0           activation_500[0][0]             \n",
            "                                                                 activation_503[0][0]             \n",
            "                                                                 activation_508[0][0]             \n",
            "                                                                 activation_509[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_528 (Conv2D)             (None, 12, 12, 160)  122880      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_514 (BatchN (None, 12, 12, 160)  480         conv2d_528[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_514 (Activation)     (None, 12, 12, 160)  0           batch_normalization_514[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_529 (Conv2D)             (None, 12, 12, 160)  179200      activation_514[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_515 (BatchN (None, 12, 12, 160)  480         conv2d_529[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_515 (Activation)     (None, 12, 12, 160)  0           batch_normalization_515[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_525 (Conv2D)             (None, 12, 12, 160)  122880      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_530 (Conv2D)             (None, 12, 12, 160)  179200      activation_515[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_511 (BatchN (None, 12, 12, 160)  480         conv2d_525[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_516 (BatchN (None, 12, 12, 160)  480         conv2d_530[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_511 (Activation)     (None, 12, 12, 160)  0           batch_normalization_511[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_516 (Activation)     (None, 12, 12, 160)  0           batch_normalization_516[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_526 (Conv2D)             (None, 12, 12, 160)  179200      activation_511[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_531 (Conv2D)             (None, 12, 12, 160)  179200      activation_516[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_512 (BatchN (None, 12, 12, 160)  480         conv2d_526[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_517 (BatchN (None, 12, 12, 160)  480         conv2d_531[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_512 (Activation)     (None, 12, 12, 160)  0           batch_normalization_512[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_517 (Activation)     (None, 12, 12, 160)  0           batch_normalization_517[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_51 (AveragePo (None, 12, 12, 768)  0           mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_524 (Conv2D)             (None, 12, 12, 192)  147456      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_527 (Conv2D)             (None, 12, 12, 192)  215040      activation_512[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_532 (Conv2D)             (None, 12, 12, 192)  215040      activation_517[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_533 (Conv2D)             (None, 12, 12, 192)  147456      average_pooling2d_51[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_510 (BatchN (None, 12, 12, 192)  576         conv2d_524[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_513 (BatchN (None, 12, 12, 192)  576         conv2d_527[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_518 (BatchN (None, 12, 12, 192)  576         conv2d_532[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_519 (BatchN (None, 12, 12, 192)  576         conv2d_533[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_510 (Activation)     (None, 12, 12, 192)  0           batch_normalization_510[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_513 (Activation)     (None, 12, 12, 192)  0           batch_normalization_513[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_518 (Activation)     (None, 12, 12, 192)  0           batch_normalization_518[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_519 (Activation)     (None, 12, 12, 192)  0           batch_normalization_519[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed5 (Concatenate)            (None, 12, 12, 768)  0           activation_510[0][0]             \n",
            "                                                                 activation_513[0][0]             \n",
            "                                                                 activation_518[0][0]             \n",
            "                                                                 activation_519[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_538 (Conv2D)             (None, 12, 12, 160)  122880      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_524 (BatchN (None, 12, 12, 160)  480         conv2d_538[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_524 (Activation)     (None, 12, 12, 160)  0           batch_normalization_524[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_539 (Conv2D)             (None, 12, 12, 160)  179200      activation_524[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_525 (BatchN (None, 12, 12, 160)  480         conv2d_539[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_525 (Activation)     (None, 12, 12, 160)  0           batch_normalization_525[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_535 (Conv2D)             (None, 12, 12, 160)  122880      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_540 (Conv2D)             (None, 12, 12, 160)  179200      activation_525[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_521 (BatchN (None, 12, 12, 160)  480         conv2d_535[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_526 (BatchN (None, 12, 12, 160)  480         conv2d_540[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_521 (Activation)     (None, 12, 12, 160)  0           batch_normalization_521[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_526 (Activation)     (None, 12, 12, 160)  0           batch_normalization_526[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_536 (Conv2D)             (None, 12, 12, 160)  179200      activation_521[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_541 (Conv2D)             (None, 12, 12, 160)  179200      activation_526[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_522 (BatchN (None, 12, 12, 160)  480         conv2d_536[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_527 (BatchN (None, 12, 12, 160)  480         conv2d_541[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_522 (Activation)     (None, 12, 12, 160)  0           batch_normalization_522[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_527 (Activation)     (None, 12, 12, 160)  0           batch_normalization_527[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_52 (AveragePo (None, 12, 12, 768)  0           mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_534 (Conv2D)             (None, 12, 12, 192)  147456      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_537 (Conv2D)             (None, 12, 12, 192)  215040      activation_522[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_542 (Conv2D)             (None, 12, 12, 192)  215040      activation_527[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_543 (Conv2D)             (None, 12, 12, 192)  147456      average_pooling2d_52[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_520 (BatchN (None, 12, 12, 192)  576         conv2d_534[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_523 (BatchN (None, 12, 12, 192)  576         conv2d_537[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_528 (BatchN (None, 12, 12, 192)  576         conv2d_542[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_529 (BatchN (None, 12, 12, 192)  576         conv2d_543[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_520 (Activation)     (None, 12, 12, 192)  0           batch_normalization_520[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_523 (Activation)     (None, 12, 12, 192)  0           batch_normalization_523[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_528 (Activation)     (None, 12, 12, 192)  0           batch_normalization_528[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_529 (Activation)     (None, 12, 12, 192)  0           batch_normalization_529[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed6 (Concatenate)            (None, 12, 12, 768)  0           activation_520[0][0]             \n",
            "                                                                 activation_523[0][0]             \n",
            "                                                                 activation_528[0][0]             \n",
            "                                                                 activation_529[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_548 (Conv2D)             (None, 12, 12, 192)  147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_534 (BatchN (None, 12, 12, 192)  576         conv2d_548[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_534 (Activation)     (None, 12, 12, 192)  0           batch_normalization_534[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_549 (Conv2D)             (None, 12, 12, 192)  258048      activation_534[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_535 (BatchN (None, 12, 12, 192)  576         conv2d_549[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_535 (Activation)     (None, 12, 12, 192)  0           batch_normalization_535[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_545 (Conv2D)             (None, 12, 12, 192)  147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_550 (Conv2D)             (None, 12, 12, 192)  258048      activation_535[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_531 (BatchN (None, 12, 12, 192)  576         conv2d_545[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_536 (BatchN (None, 12, 12, 192)  576         conv2d_550[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_531 (Activation)     (None, 12, 12, 192)  0           batch_normalization_531[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_536 (Activation)     (None, 12, 12, 192)  0           batch_normalization_536[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_546 (Conv2D)             (None, 12, 12, 192)  258048      activation_531[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_551 (Conv2D)             (None, 12, 12, 192)  258048      activation_536[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_532 (BatchN (None, 12, 12, 192)  576         conv2d_546[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_537 (BatchN (None, 12, 12, 192)  576         conv2d_551[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_532 (Activation)     (None, 12, 12, 192)  0           batch_normalization_532[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_537 (Activation)     (None, 12, 12, 192)  0           batch_normalization_537[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_53 (AveragePo (None, 12, 12, 768)  0           mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_544 (Conv2D)             (None, 12, 12, 192)  147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_547 (Conv2D)             (None, 12, 12, 192)  258048      activation_532[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_552 (Conv2D)             (None, 12, 12, 192)  258048      activation_537[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_553 (Conv2D)             (None, 12, 12, 192)  147456      average_pooling2d_53[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_530 (BatchN (None, 12, 12, 192)  576         conv2d_544[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_533 (BatchN (None, 12, 12, 192)  576         conv2d_547[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_538 (BatchN (None, 12, 12, 192)  576         conv2d_552[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_539 (BatchN (None, 12, 12, 192)  576         conv2d_553[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_530 (Activation)     (None, 12, 12, 192)  0           batch_normalization_530[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_533 (Activation)     (None, 12, 12, 192)  0           batch_normalization_533[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_538 (Activation)     (None, 12, 12, 192)  0           batch_normalization_538[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_539 (Activation)     (None, 12, 12, 192)  0           batch_normalization_539[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed7 (Concatenate)            (None, 12, 12, 768)  0           activation_530[0][0]             \n",
            "                                                                 activation_533[0][0]             \n",
            "                                                                 activation_538[0][0]             \n",
            "                                                                 activation_539[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_556 (Conv2D)             (None, 12, 12, 192)  147456      mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_542 (BatchN (None, 12, 12, 192)  576         conv2d_556[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_542 (Activation)     (None, 12, 12, 192)  0           batch_normalization_542[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_557 (Conv2D)             (None, 12, 12, 192)  258048      activation_542[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_543 (BatchN (None, 12, 12, 192)  576         conv2d_557[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_543 (Activation)     (None, 12, 12, 192)  0           batch_normalization_543[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_554 (Conv2D)             (None, 12, 12, 192)  147456      mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_558 (Conv2D)             (None, 12, 12, 192)  258048      activation_543[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_540 (BatchN (None, 12, 12, 192)  576         conv2d_554[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_544 (BatchN (None, 12, 12, 192)  576         conv2d_558[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_540 (Activation)     (None, 12, 12, 192)  0           batch_normalization_540[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_544 (Activation)     (None, 12, 12, 192)  0           batch_normalization_544[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_555 (Conv2D)             (None, 5, 5, 320)    552960      activation_540[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_559 (Conv2D)             (None, 5, 5, 192)    331776      activation_544[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_541 (BatchN (None, 5, 5, 320)    960         conv2d_555[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_545 (BatchN (None, 5, 5, 192)    576         conv2d_559[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_541 (Activation)     (None, 5, 5, 320)    0           batch_normalization_541[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_545 (Activation)     (None, 5, 5, 192)    0           batch_normalization_545[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_23 (MaxPooling2D) (None, 5, 5, 768)    0           mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "mixed8 (Concatenate)            (None, 5, 5, 1280)   0           activation_541[0][0]             \n",
            "                                                                 activation_545[0][0]             \n",
            "                                                                 max_pooling2d_23[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_564 (Conv2D)             (None, 5, 5, 448)    573440      mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_550 (BatchN (None, 5, 5, 448)    1344        conv2d_564[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_550 (Activation)     (None, 5, 5, 448)    0           batch_normalization_550[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_561 (Conv2D)             (None, 5, 5, 384)    491520      mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_565 (Conv2D)             (None, 5, 5, 384)    1548288     activation_550[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_547 (BatchN (None, 5, 5, 384)    1152        conv2d_561[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_551 (BatchN (None, 5, 5, 384)    1152        conv2d_565[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_547 (Activation)     (None, 5, 5, 384)    0           batch_normalization_547[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_551 (Activation)     (None, 5, 5, 384)    0           batch_normalization_551[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_562 (Conv2D)             (None, 5, 5, 384)    442368      activation_547[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_563 (Conv2D)             (None, 5, 5, 384)    442368      activation_547[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_566 (Conv2D)             (None, 5, 5, 384)    442368      activation_551[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_567 (Conv2D)             (None, 5, 5, 384)    442368      activation_551[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_54 (AveragePo (None, 5, 5, 1280)   0           mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_560 (Conv2D)             (None, 5, 5, 320)    409600      mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_548 (BatchN (None, 5, 5, 384)    1152        conv2d_562[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_549 (BatchN (None, 5, 5, 384)    1152        conv2d_563[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_552 (BatchN (None, 5, 5, 384)    1152        conv2d_566[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_553 (BatchN (None, 5, 5, 384)    1152        conv2d_567[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_568 (Conv2D)             (None, 5, 5, 192)    245760      average_pooling2d_54[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_546 (BatchN (None, 5, 5, 320)    960         conv2d_560[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_548 (Activation)     (None, 5, 5, 384)    0           batch_normalization_548[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_549 (Activation)     (None, 5, 5, 384)    0           batch_normalization_549[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_552 (Activation)     (None, 5, 5, 384)    0           batch_normalization_552[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_553 (Activation)     (None, 5, 5, 384)    0           batch_normalization_553[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_554 (BatchN (None, 5, 5, 192)    576         conv2d_568[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_546 (Activation)     (None, 5, 5, 320)    0           batch_normalization_546[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed9_0 (Concatenate)          (None, 5, 5, 768)    0           activation_548[0][0]             \n",
            "                                                                 activation_549[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_12 (Concatenate)    (None, 5, 5, 768)    0           activation_552[0][0]             \n",
            "                                                                 activation_553[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_554 (Activation)     (None, 5, 5, 192)    0           batch_normalization_554[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed9 (Concatenate)            (None, 5, 5, 2048)   0           activation_546[0][0]             \n",
            "                                                                 mixed9_0[0][0]                   \n",
            "                                                                 concatenate_12[0][0]             \n",
            "                                                                 activation_554[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_573 (Conv2D)             (None, 5, 5, 448)    917504      mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_559 (BatchN (None, 5, 5, 448)    1344        conv2d_573[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_559 (Activation)     (None, 5, 5, 448)    0           batch_normalization_559[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_570 (Conv2D)             (None, 5, 5, 384)    786432      mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_574 (Conv2D)             (None, 5, 5, 384)    1548288     activation_559[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_556 (BatchN (None, 5, 5, 384)    1152        conv2d_570[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_560 (BatchN (None, 5, 5, 384)    1152        conv2d_574[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_556 (Activation)     (None, 5, 5, 384)    0           batch_normalization_556[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_560 (Activation)     (None, 5, 5, 384)    0           batch_normalization_560[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_571 (Conv2D)             (None, 5, 5, 384)    442368      activation_556[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_572 (Conv2D)             (None, 5, 5, 384)    442368      activation_556[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_575 (Conv2D)             (None, 5, 5, 384)    442368      activation_560[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_576 (Conv2D)             (None, 5, 5, 384)    442368      activation_560[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_55 (AveragePo (None, 5, 5, 2048)   0           mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_569 (Conv2D)             (None, 5, 5, 320)    655360      mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_557 (BatchN (None, 5, 5, 384)    1152        conv2d_571[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_558 (BatchN (None, 5, 5, 384)    1152        conv2d_572[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_561 (BatchN (None, 5, 5, 384)    1152        conv2d_575[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_562 (BatchN (None, 5, 5, 384)    1152        conv2d_576[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_577 (Conv2D)             (None, 5, 5, 192)    393216      average_pooling2d_55[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_555 (BatchN (None, 5, 5, 320)    960         conv2d_569[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_557 (Activation)     (None, 5, 5, 384)    0           batch_normalization_557[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_558 (Activation)     (None, 5, 5, 384)    0           batch_normalization_558[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_561 (Activation)     (None, 5, 5, 384)    0           batch_normalization_561[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_562 (Activation)     (None, 5, 5, 384)    0           batch_normalization_562[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_563 (BatchN (None, 5, 5, 192)    576         conv2d_577[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_555 (Activation)     (None, 5, 5, 320)    0           batch_normalization_555[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed9_1 (Concatenate)          (None, 5, 5, 768)    0           activation_557[0][0]             \n",
            "                                                                 activation_558[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_13 (Concatenate)    (None, 5, 5, 768)    0           activation_561[0][0]             \n",
            "                                                                 activation_562[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_563 (Activation)     (None, 5, 5, 192)    0           batch_normalization_563[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed10 (Concatenate)           (None, 5, 5, 2048)   0           activation_555[0][0]             \n",
            "                                                                 mixed9_1[0][0]                   \n",
            "                                                                 concatenate_13[0][0]             \n",
            "                                                                 activation_563[0][0]             \n",
            "==================================================================================================\n",
            "Total params: 21,802,784\n",
            "Trainable params: 21,768,352\n",
            "Non-trainable params: 34,432\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ih0btulGxoDZ",
        "colab_type": "text"
      },
      "source": [
        "# Residual Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZY3qyrYXx6zs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        },
        "outputId": "95e5a012-f36e-4e0c-8922-40f3ff3601c9"
      },
      "source": [
        "from keras import layers, Input\n",
        "\n",
        "x = Input(batch_shape=(1000, 32, 32, 256))\n",
        "\n",
        "y = layers.Conv2D(128, 3, activation='relu', padding='same')(x) # by default, strides=1 and no padding\n",
        "z = layers.Conv2D(128, 3, activation='relu', padding='same')(y)\n",
        "print(z.shape) # (1000, 32, 32, 128)\n",
        "t = layers.MaxPool2D(pool_size=2, strides=2)(z) # by default, strides=pool_size\n",
        "print(t.shape) # (1000, 16, 16, 128)\n",
        "\n",
        "# linear transformation (use strides=2 for sampling)\n",
        "residual = layers.Conv2D(128, 1, strides=2, padding='same')(x)\n",
        "print(residual.shape) # (1000, 16, 16, 128)\n",
        "\n",
        "op = layers.add([t, residual])\n",
        "print(op.shape) # (1000, 16, 16, 128)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1000, 32, 32, 128)\n",
            "(1000, 16, 16, 128)\n",
            "(1000, 16, 16, 128)\n",
            "(1000, 16, 16, 128)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVJP0s54fMN8",
        "colab_type": "text"
      },
      "source": [
        "# Siamese CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfOboeJePfaV",
        "colab_type": "text"
      },
      "source": [
        "Double camera:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ef1yE0XSH86g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 65
        },
        "outputId": "9cbca357-38be-491f-9b40-61b63e1fd727"
      },
      "source": [
        "from keras import layers\n",
        "from keras import applications\n",
        "from keras import Input\n",
        "\n",
        "# 我們使用 Xception 神經網路的卷積基底 (不包含最上層的分類器) 進行影像的特徵萃取\n",
        "xception_base = applications.Xception(weights=None, include_top=False)\n",
        "\n",
        "# 建立左、右輸入張量 (左、右鏡頭影像), 其 shape 為 (250, 250, 3), 即為 250x250 的彩色影像。\n",
        "left_input = Input(shape=(250, 250, 3))\n",
        "right_input = Input(shape=(250, 250, 3))\n",
        "\n",
        "# 呼叫相同的視覺模型兩次, 也就是將影像張量傳入 Xception 神經網路物件。\n",
        "left_features = xception_base(left_input)\n",
        "right_features = xception_base(right_input)\n",
        "\n",
        "# 萃取出的左、右影像特徵張量 shape = (?, 8, 8, 2048)\n",
        "print(left_features.shape)\n",
        "print(right_features.shape)\n",
        "\n",
        "# 串接左右影像特徵張量, shape = (?, 8, 8, 4096)\n",
        "merged_features = layers.concatenate([left_features, right_features], axis=-1)\n",
        "print(merged_features.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(None, 8, 8, 2048)\n",
            "(None, 8, 8, 2048)\n",
            "(None, 8, 8, 4096)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kf3xj55hLb7N",
        "colab_type": "text"
      },
      "source": [
        "# Siamese LSTM "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7LIUr3JOMDw",
        "colab_type": "text"
      },
      "source": [
        "Example: A model for evaluating the similarity between two sentences.\n",
        "\n",
        "Since the relation between sentence A and sentence B is the same as B and A, we don't need to train two independent LSTM for each sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oj2b2pp8L_CQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32
        },
        "outputId": "e13dc4c2-2a6c-4417-a9e3-c3f7efbe7e42"
      },
      "source": [
        "from keras import layers, Input\n",
        "from keras.models import Model\n",
        "\n",
        "lstm = layers.LSTM(32)\n",
        "\n",
        "left_input = Input(shape=(None, 128))\n",
        "right_input = Input(shape=(None, 128))\n",
        "\n",
        "left_output = lstm(left_input) # (?, 32)\n",
        "right_output = lstm(right_input) # (?, 32)\n",
        "\n",
        "merged = layers.concatenate([left_output, right_output], axis=-1)\n",
        "print(merged.shape) # (None, 64)\n",
        "\n",
        "predictions = layers.Dense(1, activation='sigmoid')(merged)\n",
        "model = Model([left_input, right_input], predictions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(None, 64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlOvNq0MQj1G",
        "colab_type": "text"
      },
      "source": [
        "# Callbacks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DucSZVlQsUx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "\n",
        "def scheduler(epoch, lr):\n",
        "    if epoch < 10:\n",
        "        return lr\n",
        "    else:\n",
        "        return lr * tf.math.exp(-0.1) # e^(-0.1)\n",
        "\n",
        "callbacks_list = [\n",
        "    keras.callbacks.EarlyStopping(monitor='val_acc', patience=15), # when val_acc does not improve for more than 15 epoch, stop training\n",
        "    keras.callbacks.ModelCheckpoint(filepath='my_model.h5', monitor='val_loss', save_best_only=True),\n",
        "    keras.callbacks.ReduceLROnPlateau(monitor='val_loss', \n",
        "                                      factor=0.1, # when the callback is triggered, lr *= 0.1\n",
        "                                      patience=10), # trigger callback if val_loss doesn't decrease more than 10 epochs                  \n",
        "    keras.callbacks.LearningRateScheduler(scheduler),\n",
        "]\n",
        "\n",
        "# use model.fit(..., callbacks=callbacks_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8uIawmdYyGb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# save activations from each layer in each epoch\n",
        "class ActivationLogger(keras.callbacks.Callback):\n",
        "\n",
        "    def set_model(self, model):\n",
        "        self.model = model # tell which model should use this callback\n",
        "        layer_outputs = [layer.output for layer in model.layers]\n",
        "        # custom model that gives outputs(activations) from each layer \n",
        "        self.activations_model = keras.models.Model(model.input, layer_outputs)\n",
        "    \n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if self.validation is None:\n",
        "            raise RuntimeError('Require validation_data')\n",
        "        \n",
        "        validation_sample = self.validation_data[0][0:1]\n",
        "        activations = self.activations_model.predict(validation_sample)\n",
        "        with open(f'activation_at_epoch_{str(epoch)}.npz', 'w') as f:\n",
        "            np.savez(f, activations)\n",
        "    \n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        pass\n",
        "    \n",
        "    def on_batch_begin(self, epoch, logs=None):\n",
        "        pass\n",
        "\n",
        "    def on_batch_end(self, epoch, logs=None):\n",
        "        pass\n",
        "    \n",
        "    def on_train_begin(self, epoch, logs=None):\n",
        "        pass\n",
        "\n",
        "    def on_train_end(self, epoch, logs=None):\n",
        "        pass\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05nbFD773bue",
        "colab_type": "text"
      },
      "source": [
        "# TensorBoard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2i9e_QHz3c9x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        },
        "outputId": "e13c0d74-3ea9-4e5e-82e7-e48a1844a845"
      },
      "source": [
        "import keras \n",
        "from keras import layers\n",
        "from keras.datasets import imdb\n",
        "from keras.preprocessing import sequence\n",
        "\n",
        "max_features = 2000\n",
        "max_len = 500\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=max_len)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=max_len)\n",
        "\n",
        "model = keras.models.Sequential()\n",
        "model.add(layers.Embedding(max_features, 128, input_length=max_len, name='embed'))\n",
        "model.add(layers.Conv1D(32, 7, activation='relu'))\n",
        "model.add(layers.MaxPool1D(5))\n",
        "model.add(layers.Conv1D(32, 7, activation='relu'))\n",
        "model.add(layers.GlobalMaxPool1D())\n",
        "model.add(layers.Dense(1))\n",
        "model.summary()\n",
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embed (Embedding)            (None, 500, 128)          256000    \n",
            "_________________________________________________________________\n",
            "conv1d (Conv1D)              (None, 494, 32)           28704     \n",
            "_________________________________________________________________\n",
            "max_pooling1d (MaxPooling1D) (None, 98, 32)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 92, 32)            7200      \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d (Global (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 291,937\n",
            "Trainable params: 291,937\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7m7KofsAz0s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir ./drive/\"My Drive\"/my_log_dir"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVwJJ0DN6tJy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 761
        },
        "outputId": "dcc69867-2784-4b89-c126-21a873d1b366"
      },
      "source": [
        "callbacks = [\n",
        "    keras.callbacks.TensorBoard(\n",
        "        log_dir='./drive/My Drive/my_log_dir', \n",
        "        histogram_freq=1, # draw the activation histogram every 1 epoch\n",
        "        embeddings_freq=1 # draw the 3D embeddings every 1 epoch\n",
        "    )\n",
        "]\n",
        "\n",
        "history = model.fit(x_train, y_train, \n",
        "                    epochs=20, batch_size=128, \n",
        "                    validation_split=0.2, \n",
        "                    callbacks=callbacks)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "  1/157 [..............................] - ETA: 0s - loss: 1.3524 - acc: 0.4531WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
            "Instructions for updating:\n",
            "use `tf.profiler.experimental.stop` instead.\n",
            "  2/157 [..............................] - ETA: 13s - loss: 1.0437 - acc: 0.4727WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0593s vs `on_train_batch_end` time: 0.1085s). Check your callbacks.\n",
            "157/157 [==============================] - 8s 49ms/step - loss: 0.5879 - acc: 0.6944 - val_loss: 0.4286 - val_acc: 0.8338\n",
            "Epoch 2/20\n",
            "157/157 [==============================] - 7s 46ms/step - loss: 0.4245 - acc: 0.8508 - val_loss: 0.4545 - val_acc: 0.8316\n",
            "Epoch 3/20\n",
            "157/157 [==============================] - 7s 47ms/step - loss: 0.3560 - acc: 0.8820 - val_loss: 0.4603 - val_acc: 0.8632\n",
            "Epoch 4/20\n",
            "157/157 [==============================] - 7s 46ms/step - loss: 0.3305 - acc: 0.8999 - val_loss: 0.4960 - val_acc: 0.8610\n",
            "Epoch 5/20\n",
            "157/157 [==============================] - 7s 47ms/step - loss: 0.2956 - acc: 0.9191 - val_loss: 0.5703 - val_acc: 0.8680\n",
            "Epoch 6/20\n",
            "157/157 [==============================] - 7s 46ms/step - loss: 0.2424 - acc: 0.9396 - val_loss: 0.5946 - val_acc: 0.8650\n",
            "Epoch 7/20\n",
            "157/157 [==============================] - 7s 47ms/step - loss: 0.2001 - acc: 0.9565 - val_loss: 0.7384 - val_acc: 0.8642\n",
            "Epoch 8/20\n",
            "157/157 [==============================] - 7s 46ms/step - loss: 0.1829 - acc: 0.9689 - val_loss: 0.7789 - val_acc: 0.8668\n",
            "Epoch 9/20\n",
            "157/157 [==============================] - 7s 46ms/step - loss: 0.1323 - acc: 0.9813 - val_loss: 0.9262 - val_acc: 0.8592\n",
            "Epoch 10/20\n",
            "157/157 [==============================] - 7s 47ms/step - loss: 0.1161 - acc: 0.9854 - val_loss: 0.9272 - val_acc: 0.8682\n",
            "Epoch 11/20\n",
            "157/157 [==============================] - 7s 46ms/step - loss: 0.1057 - acc: 0.9895 - val_loss: 1.0921 - val_acc: 0.8532\n",
            "Epoch 12/20\n",
            "157/157 [==============================] - 7s 46ms/step - loss: 0.0983 - acc: 0.9891 - val_loss: 1.0910 - val_acc: 0.8648\n",
            "Epoch 13/20\n",
            "157/157 [==============================] - 7s 46ms/step - loss: 0.0934 - acc: 0.9907 - val_loss: 1.1580 - val_acc: 0.8594\n",
            "Epoch 14/20\n",
            "157/157 [==============================] - 7s 46ms/step - loss: 0.0918 - acc: 0.9916 - val_loss: 1.1664 - val_acc: 0.8592\n",
            "Epoch 15/20\n",
            "157/157 [==============================] - 7s 47ms/step - loss: 0.0909 - acc: 0.9908 - val_loss: 1.2325 - val_acc: 0.8634\n",
            "Epoch 16/20\n",
            "157/157 [==============================] - 7s 47ms/step - loss: 0.0875 - acc: 0.9925 - val_loss: 1.3919 - val_acc: 0.8474\n",
            "Epoch 17/20\n",
            "157/157 [==============================] - 7s 45ms/step - loss: 0.0941 - acc: 0.9915 - val_loss: 1.2474 - val_acc: 0.8628\n",
            "Epoch 18/20\n",
            "157/157 [==============================] - 7s 45ms/step - loss: 0.0866 - acc: 0.9930 - val_loss: 1.2784 - val_acc: 0.8648\n",
            "Epoch 19/20\n",
            "157/157 [==============================] - 7s 45ms/step - loss: 0.0898 - acc: 0.9918 - val_loss: 1.2765 - val_acc: 0.8674\n",
            "Epoch 20/20\n",
            "157/157 [==============================] - 7s 45ms/step - loss: 0.0883 - acc: 0.9924 - val_loss: 1.3368 - val_acc: 0.8674\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzDs7w9ODpKT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# on linux\n",
        "!tensorboard --logdir=./drive/\"My Drive\"/my_log_dir"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djf1OQGYcIrK",
        "colab_type": "text"
      },
      "source": [
        "# 深度可分離卷積神經網路"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xiEm-qYectLq",
        "colab_type": "text"
      },
      "source": [
        "The depthwise separable convolution compute space convolution (3 x 3 kernel) for each input channel and concatecate them eventually. This separates the channel features from space features. Therefore, the computation cost is reduced significantly since there are less parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQBrrXNzcLgV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477
        },
        "outputId": "da55e0f6-0128-4808-e67d-2e1ccb6d685f"
      },
      "source": [
        "from keras.models import Sequential, Model\n",
        "from keras import layers\n",
        "\n",
        "height = 64\n",
        "width = 64\n",
        "channels = 3\n",
        "num_classes = 10\n",
        "\n",
        "model = Sequential()\n",
        "model.add(layers.SeparableConv2D(32, 3, \n",
        "                                 activation='relu', \n",
        "                                 input_shape=(height, width, channels)))\n",
        "model.add(layers.SeparableConv2D(64, 3, activation='relu'))\n",
        "model.add(layers.MaxPool2D(2))\n",
        "\n",
        "model.add(layers.SeparableConv2D(64, 3, activation='relu'))\n",
        "model.add(layers.SeparableConv2D(128, 3, activation='relu'))\n",
        "model.add(layers.MaxPooling2D(2))\n",
        "\n",
        "#model.add(layers.SeparableConv2D(64, 3, activation='relu'))  # 怪怪的\n",
        "model.add(layers.SeparableConv2D(128, 3, activation='relu'))\n",
        "model.add(layers.GlobalAveragePooling2D()) # 對張量一、二軸取最大值\n",
        "\n",
        "model.add(layers.Dense(32, activation='relu'))\n",
        "model.add(layers.Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
        "model.summary()"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_14\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "separable_conv2d_67 (Separab (None, 62, 62, 32)        155       \n",
            "_________________________________________________________________\n",
            "separable_conv2d_68 (Separab (None, 60, 60, 64)        2400      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_26 (MaxPooling (None, 30, 30, 64)        0         \n",
            "_________________________________________________________________\n",
            "separable_conv2d_69 (Separab (None, 28, 28, 64)        4736      \n",
            "_________________________________________________________________\n",
            "separable_conv2d_70 (Separab (None, 26, 26, 128)       8896      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_27 (MaxPooling (None, 13, 13, 128)       0         \n",
            "_________________________________________________________________\n",
            "separable_conv2d_71 (Separab (None, 11, 11, 128)       17664     \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_13  (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_27 (Dense)             (None, 32)                4128      \n",
            "_________________________________________________________________\n",
            "dense_28 (Dense)             (None, 10)                330       \n",
            "=================================================================\n",
            "Total params: 38,309\n",
            "Trainable params: 38,309\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
